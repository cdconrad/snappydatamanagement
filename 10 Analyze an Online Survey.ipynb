{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze an Online Survey\n",
    "*Â© 2023 Colin Conrad*\n",
    "\n",
    "**In this notebook, we will achieve the following objectives:**\n",
    "- Prepare data and conduct descriptive analysis\n",
    "- Make an inference from the data\n",
    "- Dig into the app use data\n",
    "- Visualize correlations\n",
    "- Run regression analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare data and conduct descriptive analysis\n",
    "When completing a study that requires inferential statistics, it is important to start by preparing your data and analyzing the distributions. We can start by loading the data into our Python environment, as we did the past two weeks. The only difference from what you have seen before is that we will also import _scipy_. This library is a package made specifically for scientific analysis in Python, and we will use this to conduct our statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats # the scipy stats module\n",
    "\n",
    "sns.set_theme() # set seaborn\n",
    "\n",
    "raw_data = pd.read_csv(\"data/10_covid_apps.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data\n",
    "Let's start by describing the data. The first thing that you will notice is that the data consists of a series of answers to questions. These data fall into three types:\n",
    "- Nominal variables (e.g. gender has the values [\"Man\", \"Woman\", \"Non-binary\" and \"Prefer not to disclose\"])\n",
    "- Binary variables (e.g. Q3_tinder has the values [0,1] denoting whether the user used Tinder)\n",
    "- Continuous variables (e.g. Q13 has values from 1 to 7, denoting a Likert scale from \"strongly disagree\" to \"strongly agree\".\n",
    "\n",
    "We can use `describe()` to interpret the binary and continuous responses. We will see that the data is structured with labels that are not terribly helpful. This is because they were labelled this way in Qualtrics, the software used to collect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean these up a bit. Using a new data frame called `processed` we can assign more meaningful headers to the data, rather than `Q1, Q2... etc`. The code below adds various new labels to the processed data frame. Comments are given to help you interpret the data. We also generate the data `head` at the end of the block for context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = pd.DataFrame() # create a new data frame for processed data\n",
    "\n",
    "# What is your age? \n",
    "processed['age'] = raw_data['Q1']\n",
    "\n",
    "# Which of the following best describes your gender\n",
    "processed['gender'] = raw_data['Q2']\n",
    "\n",
    "# Which of the following dating apps have you used?\n",
    "processed['apps_used'] = raw_data['Q3']\n",
    "\n",
    "# Q4 is missing because it was a dummy used for instructional purposes\n",
    "\n",
    "# What are main purposes that motivate you to use dating apps currently or in the past?\n",
    "processed['apps_purposes'] = raw_data['Q5']\n",
    "\n",
    "# Of the matches with other people generated by the dating apps that you have used, which percentage of matches result in a conversation\n",
    "processed['apps_conversations'] = raw_data['Q6']\n",
    "\n",
    "# Of the matches with other people that resulted in conversations, how often did the match result in an in-person date? \n",
    "processed['conversations_in_person'] = raw_data['Q7']\n",
    "\n",
    "# It is easy to meet dates using a dating app (strongly disagree to strongly agree).\n",
    "processed['app_ease'] = raw_data['Q13']\n",
    "\n",
    "# It is easy to meet dates when I am in physical group settings (e.g. parties, bars, restaurants) (strongly disagree to strongly agree)\n",
    "processed['physical_ease'] = raw_data['Q20']\n",
    "\n",
    "# It is easy to meet dates when I am in an online community (strongly disagree to strongly agree).\n",
    "processed['online_ease'] = raw_data['Q27']\n",
    "\n",
    "# Due to COVID-19 I am [insert method here] more than I used to.\n",
    "processed['covid_apps_more'] = raw_data['Q28']\n",
    "processed['covid_physical_more'] = raw_data['Q29']\n",
    "processed['covid_online_more'] = raw_data['Q30']\n",
    "\n",
    "# Note: questions 8 - 12, 14-19, and 21-26 concerned social presence, trust and dating apps, excluded from this week's lab\n",
    "\n",
    "processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that you will noticed is that some of these questions were multi-select (e.g. `apps_used`). We will cover these in Objective 3, in an effort to keep this simpler. Just know that I broke these into binary variables to make it possible to interpret the data. This processing was done on Excel for simplicity.\n",
    "\n",
    "Finally, it is usually a good practice to drop the responses that were missing at least one of the questions. Pandas lets us do this with the `.dropna()` method. In this dataset, there were three missing responses, so data from three participants are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all participants who did not answer at least one question\n",
    "processed = processed.dropna()\n",
    "\n",
    "processed.describe() # 197 responses remain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring distributions\n",
    "One of the most valuable types of analysis that we can do is explore descriptive statistics. Descriptive statistics (like descriptive analytics) explores the data and provides insights about it. Tools like mean, median and mode (described in Lab 5) are examples of descriptive statistics. \n",
    "\n",
    "Another method commonly used are histograms. Similar to bar charts, histograms visualize nominal values on one axis and their measure on the other. However unlike bar graphs, their measures are data frequencies and their nominal values are collections called `bins`. In the table below, we visualize seven bins-- one for each of the values of the Likert scale on the \"ease of finding dates\" in physical group settings. We see a distribution that looks close to the \"normal\" distribution, which is interesting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ease of finding dates in physical group settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed['physical_ease'].hist(bins=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of ages\n",
    "We can also easily create histograms to observe the frequencies of predefined nominal responses, such as the age and gender categories. However, as Python isn't that smart, and is not able to figure out the numerical order of these strings. If we use the `sort_values` method to re-order the data, it will tell a clearer picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = processed['age'].sort_values(ascending=True)\n",
    "sorted_data.hist(bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of gender\n",
    "Finally, we can also observe the values for the gender responses. This data skewed `man`, and there were a couple of responses that were infrequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed['gender'].hist(bins=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining histograms\n",
    "Finally, we can also compare two histograms on one graph, to understand whether their distributions were different. In the graph below, we compare the \"ease of finding dates in physical group settings\" with the \"ease of finding dates in online communities\". We can see that the distributions are quite different. As we will see in Objective 2, these distributions are statistically different, leading us to infer that respondents were more likely to believe that it is harder to find dates in online communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = processed[['physical_ease','online_ease']]\n",
    "comparison.plot.hist(bins=7, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Make an inference from the data\n",
    "Descriptive statistics are certainly interesting, though they can only get us so far. Much of the power of scientific reasoning comes from _inference_ or the ability to infer something from the data. \n",
    "\n",
    "At first glance, it can be tempting to simply infer that people find it find dates in person than when using dating apps. After all, the mean response was _higher_ for the question about meeting in person! But how do you know that this wasn't due to random chance? Are you sure you can make this inference? \n",
    "\n",
    "There's a whole discipline (arguably many disciplines) devoted to answering this exact question, and answering this is outside of the scope of the course. However, we can still explore a few things so that you can get a taste of how this science works.\n",
    "\n",
    "### The 95% margin of error and the t-test\n",
    "In this study, we sampled responses from 200 participants, which is a sizable number, but will not perfectly represent the whole population. Instead, we have a _sample_ of the real value. It would be desirable to know whether the differences observed in our data accurately represent the real world.\n",
    "\n",
    "Fortunately, statisticians have developed just such techniques and they revolve around _confidence intervals_. Essentially, confidence intervals allow us to know, at a specified level of certainty, whether the data will fall within a range. In social sciences, we use a 95% threshold to determine an acceptable level of certainty. In other words, using high and low confidence intervals we can know with 95% certainty where the real value of a mean lies. This is why when you hear about political polls, they always give the qualifier: \"between XX and YY values, 19 times out of 20\"-- this is a confidence interval!\n",
    "\n",
    "There is a lot we can do with confidence intervals, though perhaps most interestingly, you can use them to confidently infer that _two phenomena are different_. This is essentially what Student's t-test does; it compares the distributions of two collections of data and determines the probability that they came from the same source. Using this technique, we can make inferences from our data.\n",
    "#### Research Question: Do people perceive it as more difficult to find dates using dating apps than through groups or online communities?\n",
    "This leads us to a research question. In this dataset, we explored whether people perceived it as easier to find dates using dating apps than through physical groups or online communities. We can answer this question using confidence intervals and the t-test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating mean and confidence intervals (CI)\n",
    "Let's start by calculating mean and confidence intervals. Scipy is a library (or [ecosystem, according to them](https://www.scipy.org/)) that  provides many libraries that support scientific analysis. The scipy stats library has a lot of methods that will do the heavy lifting for us. For instance, we can use `stats.sem()` to calculate the standard error. The catch is that this environment does not like data frames, so we will have to convert it to numpy.\n",
    "\n",
    "The function below can help further simplify all of this. This handy function was originally generated by [petezurich (2022)](https://stackoverflow.com/questions/15033511/compute-a-confidence-interval-from-sample-data) on Stackoverflow. I added a few comments to help explain it. We can use this to calculate the confidence intervals of our \"ease of finding dates\" responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data) # convert our data frame into a numpy array\n",
    "    n = len(a) # get the number of observations\n",
    "    \n",
    "     # mean and standard error are calculated using scipy\n",
    "    m, se = np.mean(a), stats.sem(a)\n",
    "    \n",
    "    # the ppf function is for calculating 'percent point' and saves it as h (height)\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    \n",
    "    return m, m-h, m+h # mean, low bound CI, high bound CI\n",
    "\n",
    "\n",
    "# call the function three times for each of the columns of interest\n",
    "app_ease = mean_confidence_interval(processed['app_ease'])\n",
    "physical_ease = mean_confidence_interval(processed['physical_ease'])\n",
    "online_ease = mean_confidence_interval(processed['online_ease'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a table with confidence intervals\n",
    "This is good, though we now need to convert this data back into a data frame. The code below manually creates a data frame using the results from the cell above. This is an important step because we can start to see the confidence intervals. \n",
    "\n",
    "**It is clear that we cannot conclude that people perceive it as easier to find dates in physical settings, though we can conclude that they find it _harder_ in online communities**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dictionary called d\n",
    "d = {\n",
    "    \"Dating style\": [\"Dating Apps\", \"Physical Group\", \"Online Communities\"],\n",
    "    \"Ease of finding dates\": [app_ease[0], physical_ease[0], online_ease[0]],\n",
    "    \"CI Low\": [app_ease[1], physical_ease[1], online_ease[1]],\n",
    "    \"CI High\": [app_ease[2], physical_ease[2], online_ease[2]]\n",
    "}\n",
    "\n",
    "means = pd.DataFrame(data=d)  # convert d into a data frame\n",
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the confidence interval in a graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette(\"colorblind\",10) # set seaborn to the colorblind pallette\n",
    "\n",
    "# catplot is the method for making bar charts we saw last week\n",
    "g = sns.catplot(\n",
    "    data=means, kind=\"bar\",\n",
    "    x=\"Dating style\", y=\"Ease of finding dates\",\n",
    "    ci=None, height=5, aspect=1,\n",
    ")\n",
    "\n",
    "# seaborn calles these error bars an we can use them to visualize our confidence interval\n",
    "yerr = [means[\"Ease of finding dates\"] - means['CI Low'], \n",
    "        means['CI High'] - means[\"Ease of finding dates\"]]\n",
    "\n",
    "# add the error bar to our catplot\n",
    "plt.errorbar(y=means[\"Ease of finding dates\"], x=[0,1,2], yerr=yerr, fmt='none', c='black', capsize=4)\n",
    "\n",
    "# set labels\n",
    "g.set_axis_labels(\"Ease of Finding Dates (mean)\", \"Dating Style\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct t-tests to determine probabilities\n",
    "Though we can visually observe the confidence intervals, it can also be valuable to conduct the standard t-test. In this case, it is most appropriate to use a test called a _paired t-test_ which we can borrow from the `scipy stats` library. As mentioned above, in this context the t-test tells us the t-statistic (a statistic about the difference between the data which we can ignore in this class) and the probability (a.k.a. `pvalue`) that two sample values have the same mean in reality. \n",
    "\n",
    "In the code below, we run three t-test comparisons:\n",
    "- The probability that `app_ease` and `online_ease` have the same mean (less than 1 in a million chance)\n",
    "- The probability that `physical_ease` and `online_ease` have the same mean (less than 1 in a million chance)\n",
    "- The probability that `app_ease` and `physical_ease` have the same mean (36%)\n",
    "\n",
    "In the case of the last comparison, we cannot conclude that the values are actually different. However, we can safely conclude that people perceive it as more difficult to find dates in online communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_rel(processed['app_ease'], processed['online_ease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_rel(processed['physical_ease'], processed['online_ease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_rel(processed['app_ease'], processed['physical_ease'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dig into the app use data\n",
    "That last objective was a doozie. Fortunately, we only have three objectives this week. In this final objective, we are going to dig into that app use data discussed earlier. As you recall, we collected multi-response questions from participants about the apps that they have used in the past. We then broke these into binary variables. The code below will add these values to your `processed` data frame. There's a lot we can do with this data, both descriptive and inferential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the apps that were used\n",
    "processed['used_tinder'] = raw_data['Q3_tinder']\n",
    "processed['used_bumble'] = raw_data['Q3_bumble']\n",
    "processed['used_match'] = raw_data['Q3_match']\n",
    "processed['used_pof'] = raw_data['Q3_pof']\n",
    "processed['used_okcupid'] = raw_data['Q3_okcupid']\n",
    "processed['used_hinge'] = raw_data['Q3_hinge']\n",
    "processed['used_clover'] = raw_data['Q3_clover']\n",
    "processed['used_grindr'] = raw_data['Q3_grindr']\n",
    "processed['used_coffeebean'] = raw_data['Q3_coffeebean']\n",
    "processed['used_meetme'] = raw_data['Q3_meetme']\n",
    "processed['used_eharmony'] = raw_data['Q3_eharmony']\n",
    "processed['used_jda'] = raw_data['Q3_jda']\n",
    "processed['used_aff'] = raw_data['Q3_aff']\n",
    "processed['used_facebook_dating'] = raw_data['Q3_facebook_dating']\n",
    "processed['used_none'] = raw_data['Q3_never_used']\n",
    "\n",
    "# affordances ... a.k.a. uses for the app\n",
    "processed['affordance_short_term'] = raw_data['Q5_short_term']\n",
    "processed['affordance_long_term'] = raw_data['Q5_long_term']\n",
    "processed['affordance_friendship'] = raw_data['Q5_friendship']\n",
    "processed['affordance_validation'] = raw_data['Q5_validation']\n",
    "processed['affordance_entertainment'] = raw_data['Q5_entertainment']\n",
    "processed['affordance_other'] = raw_data['Q5_other']\n",
    "processed['affordance_never_used'] = raw_data['Q5_never_used']\n",
    "\n",
    "processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing most popular apps\n",
    "One of the things that we can do is visualize the response counts. In the previous steps, we could do that using histograms, though this time, histograms won't be as helpful because the variables are binary. Instead, we can simply count the number reported and visualize the results in a graph. The code below creates a fresh data frame from the counts (measured as `len()`) for each of the binary measures. It then renders the result as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dictionary with each of the apps, save the len() in the second column\n",
    "\n",
    "d = {\n",
    "        \"Dating Apps Used\": [\n",
    "            \"Tinder\", \n",
    "            \"Bumble\", \n",
    "            \"Match\",\n",
    "            \"Plenty of Fish\",\n",
    "            \"OK Cupid\",\n",
    "            \"Hinge\",\n",
    "            \"Clover\",\n",
    "            \"Grindr\",\n",
    "            \"Coffee & Bean\",\n",
    "            \"MeetMe\",\n",
    "            \"eHarmony\",\n",
    "            \"JDA\",\n",
    "            \"AFF\",\n",
    "            \"Facebook Dating\",\n",
    "            \"Never Used\"\n",
    "        ],\n",
    "        \"Number Reported\": [\n",
    "            len(processed[processed[\"used_tinder\"] == 1]),\n",
    "            len(processed[processed[\"used_bumble\"] == 1]),\n",
    "            len(processed[processed[\"used_match\"] == 1]),\n",
    "            len(processed[processed[\"used_pof\"] == 1]),\n",
    "            len(processed[processed[\"used_okcupid\"] == 1]),\n",
    "            len(processed[processed[\"used_hinge\"] == 1]),\n",
    "            len(processed[processed[\"used_clover\"] == 1]),\n",
    "            len(processed[processed[\"used_grindr\"] == 1]),\n",
    "            len(processed[processed[\"used_coffeebean\"] == 1]),\n",
    "            len(processed[processed[\"used_meetme\"] == 1]),\n",
    "            len(processed[processed[\"used_eharmony\"] == 1]),\n",
    "            len(processed[processed[\"used_jda\"] == 1]),\n",
    "            len(processed[processed[\"used_aff\"] == 1]),\n",
    "            len(processed[processed[\"used_facebook_dating\"] == 1]),\n",
    "            len(processed[processed[\"used_none\"] == 1])\n",
    "        ]\n",
    "    }\n",
    "\n",
    "counts = pd.DataFrame(data=d) # turn this into a data frame\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the data that Tinder is the most popular app, followed by OK Cupid. If we wanted to visualize these results, we can create a `catplot` like we have so many times before. In this case, we can also remove the apps with fewer than 5% of the responses, for simplicity. This helps us understand which apps have been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_counts = counts.sort_values(by=\"Number Reported\", ascending=False) # let's sort the values\n",
    "clean_counts = clean_counts[clean_counts[\"Number Reported\"] > 10] # remove the values that are fewer than 20\n",
    "\n",
    "# create a catplot, as before\n",
    "g = sns.catplot(\n",
    "        data=clean_counts, kind=\"bar\",\n",
    "        x=\"Number Reported\", y=\"Dating Apps Used\",\n",
    "        height=5, aspect=1.25, color=\"slategray\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Question: Do Tinder users go out more during Covid-19?\n",
    "In addition to visual analytics, we can also investigate differences among people who use certain apps and those who do not. For example, we might ask whether Tinder users go out more during Covid-19. \n",
    "\n",
    "How would we do this? From a theoretical perspective, it is important to note that this type of test is different from the previous case, because we are comparing two _different_ groups of people, rather than a difference _among_ a group of people. For this type of test, we will want to use an independent t-test, rather than a paired test. This type of test is optimized for comparing two different groups.\n",
    "\n",
    "As before, we can visualize the distributions to see if there are visual differences in their responses. The following code displays the responses to the `covid_physical_more` question among tinder users and people who never used tinder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinder_users = processed[processed[\"used_tinder\"] == 1]\n",
    "tinder_users['covid_physical_more'].hist(bins=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinder_nonusers = processed[processed[\"used_tinder\"] == 0]\n",
    "tinder_nonusers['covid_physical_more'].hist(bins=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't look like there are many differences in response to this question. We can test the difference by using the independent t-test, as described. The code below implements the `ttest_ind` instead, revealing that there is probably no difference between these two samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(tinder_users['covid_physical_more'], tinder_nonusers['covid_physical_more'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualize correlations\n",
    "We've covered a lot of ground, and I don't particularly want to challenge you to _another_ question. However, there are two more topics in basic statistics that are worth discussing here. These are:\n",
    "- Correlation\n",
    "- Regression\n",
    "\n",
    "This section will highlight these two concepts interactively without bringing in more challenge questions. This will be useful for many of your final projects, as many students seem interested in investigating the relationships between variables. To demonstrate these things, we will explore the following research question:\n",
    "\n",
    "#### RQ: Is there a relationship between perceived ease of finding dates and whether people report dating more during Covid-19? \n",
    "\n",
    "### Creating aggregated measures\n",
    "Psychologists often measure _constructs_ using multiple questionnaire items. In this case, we had three items which collected information on the `ease of finding dates`. We also had three items which denoted `doing more during covid`. We can combine the `ease` measures together to get a measure of `overall_ease` which roughly measures whether people perceive it as easy to find dates in general. Likewise `overall_covid_more` is a rough measure of whether people are dating more during the Covid-19 pandemic. With these combined measures, we can have a good understanding of whether there is a relationship between people's perceived ease of finding dates and whether they are more active during Covid-19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds a new overall_ease measure\n",
    "processed['overall_ease'] = raw_data['Q13'] + raw_data['Q20'] + raw_data['Q27']\n",
    "\n",
    "# adds a new overall_covid_more measure\n",
    "processed['overall_covid_more'] = raw_data['Q28'] + raw_data['Q29'] + raw_data['Q30']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "Correlation describes a statistical common trend between two variables in either a positive or negative direction. One of the most important statistics for determining a possible relationship between two variables is called the `Pearson r`. The `r` (not to be confused with the R programming language) is a measure of statistical fit between two variables. At `r = 1`, as one variable increases, the other increases at the exact same amount. At `r=0` there is no relationship in the trend between the two variables.\n",
    "\n",
    "One of the best ways to illustrate a Pearson r is with a scatterplot. Execute the code below and you will see a visualization of the relationship between the `overall_ease` and `overall_covid_more`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=processed, x=\"overall_covid_more\", y=\"overall_ease\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the trend significant?\n",
    "You will see in this graph that as `overall_ease` increases, so too does `overall_covid_more`. This is because the two variables are correlated. From the perspective of social science, this is in fact a really nice correlation that is highly suggestive.\n",
    "\n",
    "To calculate the Pearson r using Python, you can use the `pearsonr()` method from the Scipy stats library. The code below calculates the r for us. Note that this method returns two values. The first is r, while the second is the p-value of the significance of the correlation. In this case, `p < 0.0001`, which means that it is very unlikely due to random chance that we observed this correlation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pearsonr(processed['overall_ease'], processed['overall_covid_more'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, correlation does not mean that there is causation. This is why scientific papers have discussion sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run regression analysis\n",
    "While correlation is a useful measure, we might wish to take it one step further. Similarly to correlations, `regression` is a measure of the relationship between two (or more) variables. Unlike correlation, this is a tool designed to measure a causal relationship. A simple linear regression is a way of measuring the significance of the impact of one variable on another. The `regplot` below demonstrates this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=\"overall_covid_more\", y=\"overall_ease\", data=processed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the impact\n",
    "This type of regression is simply a calculation of a straight line that best fits the Pearson correlation (known as _ordinary least squares_). Essentially, it quantifies the size of the calculated impact of `overall_ease` on `overall_covid_more`. By observing this relationship we have addressed our research question. The answer is _possibly_. At least, the statistics show evidence for this... a deeper investigation might be warranted though because there are many possible explanations for this observation! \n",
    "\n",
    "The code below determines the results of a linear regression between these two variables. The results show that the relationship has a significant p-value, and could reflect a real relationship. A potential problem however is that the factors are strongly correlated, and that the survey might reflect a deeper mutual factor, such as satisfaction with dating apps. Further research would be needed to determine what factors explain this relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm # import the statsmodel library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the predictor factors of interest\n",
    "x = processed[[\n",
    "    'overall_ease'\n",
    "]]\n",
    "\n",
    "# specify the predicted factor\n",
    "y = processed['overall_covid_more']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the regression\n",
    "reg1 = sm.OLS(y, x, missing='drop')\n",
    "\n",
    "# fit the results to the model and print\n",
    "results = reg1.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct multiple linear regression analysis\n",
    "It is also possible to conduct linear regression with multiple predictive factors. Multiple linear regression allows us to take a greater degree of complexity into consideration, which can better explain real-world phenomena. The affordances data demonstrates the advantages of multiple linear regression well. We could investigate the following research question:\n",
    "\n",
    "#### RQ: What is the impact of reported affordances on whether people report dating more during Covid-19?\n",
    "\n",
    "Let's start by conducting a single factor validation once again, this time predicting the impact of the `validation` affordance on whether the users reported greater use of dating apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the predictor factors of interest\n",
    "x = processed[[\n",
    "    'affordance_validation'\n",
    "]]\n",
    "\n",
    "# specify the predicted factor\n",
    "y = processed['overall_covid_more']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the regression\n",
    "reg2 = sm.OLS(y, x, missing='drop')\n",
    "\n",
    "# fit the results to the model and print\n",
    "results = reg2.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A deceitful relationship?\n",
    "This relationship is potentially interesting. We see a significant p-value as well as an R-squared value of `0.101`. This suggests that the personal validation affordance was associated with greater dating app use. However, if we conduct a regression with the other affordances, the personal validation affordance is no longer a significant explanatory factor. This suggests that the predictive power of the validation affordance was possibly captured by the other affordances.\n",
    "\n",
    "When you run a regression with all five affordances, the resulting model is a strong predictor of increased use during Covid. This suggests that the affordances, except for validation, were likely predictors of the technology use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the predictor factors of interest\n",
    "x = processed[[\n",
    "    'affordance_friendship',\n",
    "    'affordance_long_term',\n",
    "    'affordance_validation',\n",
    "    'affordance_entertainment',\n",
    "    'affordance_other'\n",
    "]]\n",
    "\n",
    "# specify the predicted factor\n",
    "y = processed['overall_covid_more']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the regression\n",
    "reg3 = sm.OLS(y, x, missing='drop')\n",
    "\n",
    "# fit the results to the model and print\n",
    "results = reg3.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and credit exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is tricky stuff, and if you are struggling to figure it all out, you are not alone. This type of statistical analysis is used by professional social scientists to investigate the world. If you are interested in learning more, consider taking courses in statistics or pursuing advanced studies in social science. Khan Academy is a great place to start, though there are other fantastic resources available to you at free and low costs.\n",
    "\n",
    "If you are completing this exercise as part of a course, please see your learning management system for the graded exercise questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Cumming, G., & Calin-Jageman, R. (2016). _Introduction to the new statistics: Estimation, open science, and beyond_. Routledge.\n",
    "\n",
    "Diaz, G. O., and Conrad, C. (2021). Online Communities and Dating apps: The effects of social presence, trust, and Covid-19. _AMCIS 2021 Proceedings_. https://aisel.aisnet.org/amcis2021/social_computing/social_computing/2/\n",
    "\n",
    "gcamargo (August 6 2018). Compute a confidence interval from sample data. _Stack Overflow_. https://stackoverflow.com/questions/15033511/compute-a-confidence-interval-from-sample-data\n",
    "\n",
    "Khanacademy. Statistics and Probability. https://www.khanacademy.org/math/statistics-probability\n",
    "\n",
    "Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M., Reddy, T., Cournapeau, D., ... & van Mulbregt, P. (2020). SciPy 1.0: fundamental algorithms for scientific computing in Python. Nature methods, 17(3), 261-272."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
